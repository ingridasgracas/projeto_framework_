# CHECKLIST: Pr√≥ximos Passos - Pipeline em Produ√ß√£o

## üèóÔ∏è **PASSO 1: Configura√ß√£o do Ambiente de Desenvolvimento na Nuvem**

### 1.1 Configurar Google Cloud Project
```bash
# Criar projeto no Google Cloud Console
gcloud projects create pipeline-saude-rj-dev
gcloud config set project pipeline-saude-rj-dev

# Habilitar APIs necess√°rias
gcloud services enable bigquery.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable compute.googleapis.com
```

### 1.2 Criar Service Account e Credenciais
```bash
# Criar service account
gcloud iam service-accounts create pipeline-saude-sa \
    --display-name="Pipeline Saude Service Account"

# Conceder permiss√µes
gcloud projects add-iam-policy-binding pipeline-saude-rj-dev \
    --member="serviceAccount:pipeline-saude-sa@pipeline-saude-rj-dev.iam.gserviceaccount.com" \
    --role="roles/bigquery.admin"

gcloud projects add-iam-policy-binding pipeline-saude-rj-dev \
    --member="serviceAccount:pipeline-saude-sa@pipeline-saude-rj-dev.iam.gserviceaccount.com" \
    --role="roles/storage.admin"

# Gerar chave JSON
gcloud iam service-accounts keys create credentials/service-account-key.json \
    --iam-account=pipeline-saude-sa@pipeline-saude-rj-dev.iam.gserviceaccount.com
```

### 1.3 Criar Infrastructure na GCP
- [ ] **Bucket GCS**: `saude-rj-data-lake-dev`
- [ ] **Datasets BigQuery**: 
  - `saude_rj_raw`
  - `saude_rj_staging` 
  - `saude_rj_core`
  - `saude_rj_marts`

---

## üß™ **PASSO 2: Implementar Qualidade de Dados (Great Expectations)**

### 2.1 Testes de Schema
- [ ] Validar colunas obrigat√≥rias
- [ ] Verificar tipos de dados
- [ ] Validar ranges de valores

### 2.2 Testes de Neg√≥cio  
- [ ] Tempo de espera n√£o pode ser negativo
- [ ] Ocupa√ß√£o de leitos entre 0-100%
- [ ] Coordenadas dentro dos limites do RJ

---

## üöÄ **PASSO 3: Deploy do Pipeline para GCP**

### 3.1 Modificar ETL para GCP
- [ ] Enviar CSVs para Google Cloud Storage
- [ ] Carregar dados do GCS para BigQuery
- [ ] Implementar logging e monitoramento

### 3.2 Executar dbt no BigQuery
- [ ] Configurar profile do BigQuery
- [ ] Rodar `dbt run` para criar modelos
- [ ] Executar `dbt test` para valida√ß√µes

---

## ‚öôÔ∏è **PASSO 4: Automa√ß√£o com Airflow**

### 4.1 Configurar Airflow Local
- [ ] Inicializar banco do Airflow
- [ ] Configurar conex√µes (GCP, BigQuery)
- [ ] Ativar DAG do pipeline

### 4.2 Monitoramento e Alertas
- [ ] Configurar alertas de falha
- [ ] M√©tricas de performance
- [ ] Logs centralizados

---

## üìä **PASSO 5: Dashboard e Visualiza√ß√£o**

### 5.1 Looker Studio
- [ ] Conectar ao BigQuery marts
- [ ] Criar dashboards interativos
- [ ] Configurar refresh autom√°tico

---

## üéØ **Prioridade Atual**

**COME√áAR AGORA**: Configurar credenciais GCP e testar upload para Cloud Storage

Qual dessas etapas voc√™ gostaria que eu implemente primeiro?